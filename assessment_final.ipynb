{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Individual Assesment - University of the West of England\n",
    "<a href=\"https://github.com/Sp0xF8/Individual-Project\">GitHub Project Link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These imports are shared across the whole application and not specific to either model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json \n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Imports\n",
    "<p>These imports are specifically related to the SVM's functionality</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADA Ensambles\n",
    "<p>These imports are specifically related to the ADA models</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Data ready for interperating\n",
    "<p>In this stage we are using the Pandas libary to load the CSV data file.</p>\n",
    "\t<div style=\"margin-left: 20px;\">\n",
    "\t\t<p>- This helps by giving us functionality to use a wide array of methods</p>\n",
    "\t</div>\n",
    "\t<p>The data is then printed to allow for easy refencing and understanding of base data</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "2            8      183             64              0        0  23.3   \n",
      "3            1       89             66             23       94  28.1   \n",
      "4            0      137             40             35      168  43.1   \n",
      "5            5      116             74              0        0  25.6   \n",
      "6            3       78             50             32       88  31.0   \n",
      "7           10      115              0              0        0  35.3   \n",
      "8            2      197             70             45      543  30.5   \n",
      "9            8      125             96              0        0   0.0   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.351   31        0  \n",
      "2                     0.672   32        1  \n",
      "3                     0.167   21        0  \n",
      "4                     2.288   33        1  \n",
      "5                     0.201   30        0  \n",
      "6                     0.248   26        1  \n",
      "7                     0.134   29        0  \n",
      "8                     0.158   53        1  \n",
      "9                     0.232   54        1  \n",
      "(768, 9)\n"
     ]
    }
   ],
   "source": [
    "## Load the CSV file\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "print(data[:10])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of Splitting Test and Training Data in Machine Learning Models\n",
    "\n",
    "In Machine Learning, it is essential to split the dataset into training and test sets.\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>The importance of having a <strong> Validation Dataset </strong> is to have a selection of data for which the learned model will be scored against. By knowing the result of said inputs, it is possible to match them against the predictions made by the model. This is the most accurate way to test. A counter-option would be to test against the trained data, however this wouldn't be a real-world example of predicting new labels. Knowing the success rate of predicted models means the model can be adjusted until the preformance is satisfactory. \n",
    "\t</p>\n",
    "</div>\n",
    "\n",
    "Additionally, the from the training <strong>data</strong> provided by the CSV, the outcome must be dropped for the list of inputs ($X$). Everything, besides the outcome, should also be dropped from the list of outputs ($y$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the initial X and y lists from the data CSV file\n",
    "X = data.drop(\"Outcome\", axis=1)\n",
    "y = data[\"Outcome\"]\n",
    "\n",
    "\n",
    "#split the data into training and testing data, 80% training and 20% testing- random state is set to 42 because it is the answer to everything\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating SVM Variables\n",
    "These two variables are of the type Dictonary, which is similar in format to the <strong>Json</strong> file extension. Advantages of setting the models to work in this behavoiur include ease of access and increased readability. It also gives the ability to store mulitple different datatypes in one element and easily export it to a Json file.\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>svm_tests</strong> \t\t\t: This Dictonary is used to store the range of hyper-paramaters passed to the depth first grid search.\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>current_svm_data</strong> \t: This Dictionary is used to store the current hyper-paramaters being passed to the <strong>svm_model</strong> class.\n",
    "\t</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the tests which will be preformed on the SVM\n",
    "svm_tests = {\n",
    "\t'C': np.linspace(1, 100, 100),\n",
    "\t'tollerance': np.linspace(0.0001, 0.1, 100),\n",
    "\t'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "\t'max_iter': np.linspace(10, 10000, 100).astype(int),\n",
    "\t'decision_function_shape': ['ovo', 'ovr'],\n",
    "\t'possibility': [True, False],\n",
    "}\n",
    "\n",
    "# Dictionary to store the current test's data for the SVM\n",
    "current_svm_data = {\n",
    "\t'kernel': None,#\n",
    "\t'max_iter': None,\n",
    "\t'decision_function_shape': None,#\n",
    "\t'probability': None,#\n",
    "\t'shrinking': None,#\n",
    "\t'C': None,\n",
    "\t'tollerance':None#\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Ensambles Variables\n",
    "\n",
    "\n",
    "Simiarly to the previous definitions, these variables are also of the type Dictionary. They are, instead, however multi-layered. \n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>ada_ensambles_tests</strong> : This Dictonary is used to store the range of hyper-paramaters passed to the depth first grid search.\n",
    "\t\t<div style=\"margin-left: 50px;\">\n",
    "\t\t\t<p>\n",
    "\t\t\t\t<strong>Estimator</strong> \t\t\t: This Dictonary is used to store the range of hyper-paramaters passed to the <strong>Random Forest Classifier</strong>.\n",
    "\t\t\t</p>\n",
    "\t\t\t<p>\n",
    "\t\t\t\t<strong>Params</strong> \t\t\t: This Dictionary is used to store the range of hyper-paramaters passed to the <strong>Ada Boost classifier</strong>.\n",
    "\t\t\t</p>\n",
    "\t\t</div>\n",
    "\t</p>\n",
    "\t<hr style=\"width:75%;\" align=\"left\">\n",
    "\t<p>\n",
    "\t\t<strong>current_ada_data</strong> \t: This Dictionary is used to store the current hyper-paramaters being passed to the <strong>ada_model</strong> class.\n",
    "\t\t<div style=\"margin-left: 50px;\">\n",
    "\t\t\t<p>\n",
    "\t\t\t\t<strong>Estimator</strong> \t\t\t: This Dictonary is used to store the current hyper-paramaters being passed to the <strong>Random Forest Classifier</strong>.\n",
    "\t\t\t</p>\n",
    "\t\t\t<p>\n",
    "\t\t\t\t<strong>Params</strong> \t\t\t: This Dictionary is used to store the current hyper-paramaters being passed to the <strong>Ada Boost classifier</strong>.\n",
    "\t\t\t</p>\n",
    "\t\t</div>\n",
    "\t</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the tests which will be preformed on the AdaBoost Classifier and Random Forest Classifier\n",
    "ada_ensambles_tests = {\n",
    "\t'Estimator': {\n",
    "\t\t'n_estimators': np.linspace(1, 200, 10).astype(int),\n",
    "\t\t'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "\t\t'max_features': ['sqrt', 'log2'],\n",
    "\t\t'bootstrap': [True, False],\n",
    "\t\t'min_samples_split': np.linspace(2, 11, 10).astype(int),\n",
    "\t\t'min_samples_leaf': np.linspace(2, 6, 5).astype(int)\n",
    "\t},\n",
    "\t'Params': {\n",
    "\t\t'n_estimators': np.linspace(1, 100, 10).astype(int),\n",
    "\t\t'learning_rate': np.linspace(0.1, 3, 10),\n",
    "\t\t'algorithim': ['SAMME', 'SAMME.R']\n",
    "\t}\n",
    "}\n",
    "\n",
    "# Dictionary to store the current test's data for the AdaBoost Classifier and Random Forest Classifier\n",
    "current_ada_data = {\n",
    "\t'Estimator': {\n",
    "\t\t'n_estimators': None,\n",
    "\t\t'criterion': None,\n",
    "\t\t'max_features': None,\n",
    "\t\t'bootstrap': None,\n",
    "\t\t'min_samples_split': None,\n",
    "\t\t'min_samples_leaf': None\n",
    "\t},\n",
    "\t'Params': {\n",
    "\t\t'n_estimators': None,\n",
    "\t\t'learning_rate': None,\n",
    "\t\t'algorithim': None\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the test comparison\n",
    "These two vairables are of type List and are used to store the best solutions found and their respective metrics. This is essneital in preforming a goal orientated depth first search. Without having a comparison, how can you know if you are finding a better solution?\n",
    "\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>solution_list</strong> \t\t: This List stores class objects of the top 10 solutions, allowing for instant referencing once the search has been completed.\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>accuracy_list</strong> \t\t: This List stores the accuracy of the class object at the same offset and is the list used for direct comparison without having to reference the solutions list; slowing down the, already hundreds of thousands, of tests. \n",
    "\t</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_list = []\n",
    "accuracy_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratios Function\n",
    "This function is an easy way of returning multiple metrics in a more efficent way: using far less function calls.\n",
    "\n",
    "### Paramaters:\n",
    "\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>y_true</strong> \t\t: This variable is of type numpy array and holds the actual true values for the test data (<strong>y_test</strong>)\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>y_pred</strong> \t\t: This variable is also of type numpy array and holds the predicted values for the test data (<strong>y_test</strong>)\n",
    "\t</p>\n",
    "</div>\n",
    "\n",
    "### Prediction Types:\n",
    "\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>True Positive</strong> \t\t: This type of classification occurs when the model <em><strong>predicts a Positive result</strong>, and the result is <strong>actually Positive</strong></em>.\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>True Negative</strong> \t\t: This type of classification occurs when the model <em><strong>predicts a Negative result</strong>, and the result is <strong>actually Negative</strong></em>.\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>False Positive</strong> \t: This type of classification occurs when the model <em><strong>predicts a Positive result</strong>, and the result is <strong>actually Negative</strong></em>.\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>False Negative</strong> \t: This type of classification occurs when the model <em><strong>predicts a Negative result</strong>, and the result is <strong>actually Positive</strong></em>.\n",
    "\t</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funct to test the SVM model without calling different functions, this is to make the code more readable and efficient\n",
    "def ratios(y_true, y_pred):\n",
    "    ## Get the confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    \n",
    "    ## Calculate False Negative Ratio\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    \n",
    "\t## Calculate Recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "\t## Calculate Precision\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "\t## Calculate specificity\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    return fnr, recall, precision, specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Definitions\n",
    "\n",
    "Using a class instead of a standard function is highly effiecent. It allows for better reuseability, and storage, of data in the long term. This is fundamental concept of Object Orientated Programming. By using OOP in this project, it is much easier to rapidly cycle through a grid of hyper paramaters and subsequently output and results gained. This also means that less indents are reuqired to accomplish the same task, resulting in cleaner code which is much easier to understand.\n",
    "\n",
    "By choosing to use a class, there is inherent access to standardised methods: like the constructor. Both the **svm_model** and **ada_model** both use **constructurs** which accept only one paramater. These are the previously defined respective Dictionarys (**current_svm_data**, **current_ada_data**). In this regard, the constructor is used to not only setup the Machine Learning model but also used to save paramaters. These can later be referenced by a different function, printing the best solutions found by the search. \n",
    "\n",
    "Additionally, the **predict** method is shared between the classes. While they both have vastly different features, they return the same metric values. The models themselves are stored in function-local variables, meaning once the predictions have been made: they are destroyed, freeing memory. The metric data is then stored in appropriatly named class-vairables for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Variables:\n",
    "\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>closed_pred</strong> \t\t: This is a variable of type Numpy Array and holds the <em>predicted true values for the test data</em> (<strong>y_pred</strong>)\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>closed_accuracy</strong> \t: This variable is of the type Float and holds the most simple of the metrics. This is simply the number of <em>correctly classafied datapoints to the number of incorrectly classafied datapoints</em>. Having a good overall accuracy is important but, dependant on what application the model is used for, can be misappropriated.\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>closed_precision</strong> \t: This variable is also of type Float and holds a ratio of <em>correctly identified <strong>True Positives</strong> to the total number of positive predictions</em> made by the model. This is useful for analysing the validity of the positivly predicited models. \n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>closed_specificity</strong> : Similar to <strong>closed_precision</strong>, this variable is also Float. Instead, however, it stores the ratio of <em>correctly identified <strong>True Negatives</strong> to the total number of negative predictions</em> made by the model.  \n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>closed_recall</strong> \t\t: This variable type of Float is used for the <em><strong>Recall</strong>, or <strong>Sensitivity</strong>,</em> of the models predictions. This is <em>the number of <strong>True Positive</strong> predictions to the total number of positive instances</em>. Recall makes excellent pairing if <strong>closed_precision</strong> is also a metric of choice. It helps to give more complete info, as precision wil not specify how many instances were missed: only how accurate it is at finding positive predicitions. This could be misleading, as an easy way to always predict people with diabeties, without missing anyone, is to simply say <em>everyone has diabeties</em>. This would <strong><em>never miss a diabetic</strong>, but would <strong>instantly slow down formal diagnosis</strong> because everyone would be referred</em>.\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>closed_fnr</strong> \t\t: This variable is of type float and holds the <em><strong>False Negative Ratio</strong></em>. This the invserse of <strong>Recall</strong>, however, for transparancy, using this as a clear metric seemed preferable. Often refered to as the <em>miss rate</em>, this metric is highly valued when missed positive instances is critical. When dealing with any form of diagnosis, it is far more important that the model can correctly identify as many <strong>True Positives</strong> as possible. <em>While it is <strong>less than ideal if a non-diabetic is referred</strong>, this would cause the diagnosis system to slow down, it is <strong>far more important that the diabetic is not overlooked</strong> and left without treatment</em>. It is important to note that <strong>this should not be the only metric used for judgement</strong>. A good choice of pairing for this metric would also be <em>Precision or Specificity</em>. Specificity would provide a well rounded overview by providing insight into if the model is simply predicting everyone as diabetic. \n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>closed_f1</strong> \t\t\t: Of type Float, this variable stores the <em>harmonic mean of <strong>precision and recall</strong></em>. This ensures <strong>False Positives</strong> and <strong>False Negatives</strong> are accounted for, and is <em>particularly useful when neither have signifigantly different levels of importance</em>. Given the models previously stressed importance of not allowing for False Negatives, but having some leverage on False Positives: F1 scoring metrics are not entirely relavent. \n",
    "\t</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svm_model.predict Method\n",
    "\n",
    "Typically, a prediction function would allow for some way of dynamically setting the data to be predicted. However, given the time complexity of the grid searches employed, it was far more efficent to define the test data globally and reference when needed. If this was to be progressed further, the predict function would need to take a multi-dimensional array as the input (representing the data to classify). For the current scenario, it is suited perfectly and offers an optimised solution.\n",
    "\n",
    "Inside the prediction function, it begins by defining a pipline; through which, the data should be processed before reaching the **Support Vector Classifier**. Given the type of model, and the large separation of the data (*e.g pregnancies: 1, glocouse: 168), the data must be pre-processed before it can be accuratly analysed by the **SVC**. Problems can easily arise in the effectiveness of Linear based algorithms, this is where the **StandardScaler** is useful inside the pipline. Instead of predefining the the patient data (**X_train**, **X_test**, ...) after having been scaled, causing disruptions to the other ML model (**ada_model**), the data is scaled on access of the model. This means new test data added will also not need to be scaled, as the pipline will automatically handle any scaling needs. \n",
    "\n",
    "\tclosed_clf.fit(X_train, y_train)\n",
    "\n",
    "This line of code is where the SVC Pipline is called to train the model. The first paramater, **X_train**, is then passed through the pipline. First being scaled, reducing the variance in the source data, then being passed further down the pipline to the SVC model and specified kernel. At its root, the **Standard Scaler** function is designed to help convert values from different formats into a single scale. It is designed to ensure that each feature has a mean of 0 and a unit of standard deviation. This is useful for Kernels such as *RBF*, which assumes that all features are centered around a 0 origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SVM Model Definition\n",
    "class svm_model:\n",
    "\n",
    "\t## Constructor - Takes in a data dictionary (current_svm_test) and assigns the values to the class variables\n",
    "\tdef __init__(self, data):\n",
    "\t\tself.kernel \t\t\t\t\t= data['kernel']\n",
    "\t\tself.max_iter \t\t\t\t\t= data['max_iter']\n",
    "\t\tself.func_shape \t\t\t\t= data['decision_function_shape']\n",
    "\t\tself.probability \t\t\t\t= data['probability']\n",
    "\t\tself.shrinking \t\t\t\t\t= data['shrinking']\n",
    "\t\tself.tollerance \t\t\t\t= data['tollerance']\n",
    "\t\tself.C \t\t\t\t\t\t\t= data['C']\n",
    "\n",
    "\n",
    "\n",
    "\t## Predict Function - Uses the current_svm_test from the constructor to create a pipeline and predict the outcome of the test data \n",
    "\tdef predict(self):\n",
    "\n",
    "\t\t### Create the pipeline as a local variable\n",
    "\t\tclosed_clf = make_pipeline(\t\n",
    "\t\t\t\t\t\t\t\t\tStandardScaler(), # Standardise the data before training\n",
    "\t\t\t\t\t\t\t\t\tSVC( # Create the SVC model with the given parameters from the current_svm_test dictionary\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t\t\tkernel=self.kernel, \n",
    "\t\t\t\t\t\t\t\t\t\tmax_iter=self.max_iter, \n",
    "\t\t\t\t\t\t\t\t\t\tdecision_function_shape=self.func_shape, \n",
    "\t\t\t\t\t\t\t\t\t\tprobability=self.probability, \n",
    "\t\t\t\t\t\t\t\t\t\tshrinking=self.shrinking,\n",
    "\t\t\t\t\t\t\t\t\t\tC=self.C,\n",
    "\t\t\t\t\t\t\t\t\t\ttol=self.tollerance\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\t\n",
    "\t\t### Fit the model to the training data \n",
    "\t\tclosed_clf.fit(X_train, y_train)\n",
    "\n",
    "\t\t### Predict the outcome of the test data\n",
    "\t\tself.closed_pred \t\t= closed_clf.predict(X_test)\n",
    "\n",
    "\t\t### Calculate the accuracy, f1 score, false negative rate, recall, precision, and specificity of the model\n",
    "\t\tself.closed_accuracy \t= accuracy_score(y_test, self.closed_pred)\n",
    "\t\tself.closed_f1 \t\t\t= f1_score(y_test, self.closed_pred)\n",
    "\t\t\n",
    "\t\tself.closed_fnr, self.closed_recall, self.closed_precision, self.closed_specificity\t= ratios(y_test, self.closed_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ada_model.predict Method\n",
    "\n",
    "### Random Forest Classifier\n",
    "\tclosed_hype_clf = RandomForestClassifier(...)\n",
    "\n",
    "1. At its basics, **Random Forest Classifier** is a combination of multiple decision trees, all trained from a different *randonly selected* datapoint. This simply involves choosing multiple random samples from the origonal dataset. These samples are then used to build trees from. \n",
    "\n",
    "2. At *each node of each decision tree*, a random number of features is selected (*so long as the value doesnt exceed **max_features***). This helps to ensure decision trees do not run in parralel, defeating the purpose of multiple trees. \n",
    "\n",
    "3. After a node has selected its features, within these selected features: the one which provides the best split, *according to the **criterion***, is chosen. \n",
    "\n",
    "4. The remaining data is then split into subsets based on the selected feature, creating different branches on the tree. This process is then repeated at each node until a stopping crietia is met.\n",
    "\n",
    "5. After growing ***n_estimator*** number of trees, the results are aggrogated to make predictions. When these predicitons are made, each tree independently makes their own prediction. The final prediction for the *new input data, **X_test**, is the **average result of all of the trees***. \n",
    "\n",
    "### AdaBoost Classifier\n",
    "\tclosed_clf = AdaBoostClassifier(closed_hype_clf, ...)\n",
    "\n",
    "When you pass a Random Forest Classifier as an estimator to an AdaBoost Classifier, the AdaBoost algorithm works in conjunction with the Random Forest base estimator to create a strong ensamble model. Here's how it works:\n",
    "\n",
    "1. The Random Forest Classifier is initialized as the base estimator within the AdaBoost. AdaBoost uses the Random Forest Classifier to build a sequence of weak learners.\n",
    "\n",
    "2. AdaBoost trains multiple instances of the Random Forest Classifier, focusing more on the entities that were misclassified by the previous learners. This is achieved by adjusting the weights of the training entities during each iteration.\n",
    "\n",
    "3. After training multiple Random Forest models, AdaBoost combines their predictions using a weighted voting system. The final prediction is determined from the weighted sum of individual Random Forest predictions, where the weight assigned to each model depends on its performance during training.\n",
    "\n",
    "4. By iteratively focusing on difficult-to-classify entities and combining the predictions of multiple Random Forest models, AdaBoost enhances the overall performance of the ensamble classifier. This results in a robust and accurate model that utalises the strengths of both AdaBoost and Random Forest Classifiers.\n",
    "\n",
    "\n",
    "As previously mentioned, data only needs to be scaled for models which assume the data is centered around a 0 point. Neither of these classifiers do, meaning the data can be left in its standard form. This is because they make choices based on relative values, not absolute. As mentioned, they choose splits which maximise information gain: but the splits are made independently within features, meaning no scale conflicts either.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SVM Model Definition\n",
    "class ada_model:\n",
    "\n",
    "\t## Constructor - Takes in a data dictionary (current_svm_test)\n",
    "\tdef __init__(self, current_test):\n",
    "\n",
    "\t\t## Assign the values from the current_test dictionary to the class variables, which are also dictionaries. this removes the outer layer, improving readability later on\n",
    "\t\tself.clf_estimator = current_test['Estimator']\n",
    "\n",
    "\t\tself.clf_params = current_test['Params']\n",
    "\t\t\n",
    "\n",
    "\t## Predict Function - Uses the current_svm_test from the constructor to create an AdaBoost Classifier and a Random Forest Classifier to be used as an estoimator to predict the outcome of the test data\n",
    "\tdef predict(self):\n",
    "\n",
    "\t\t### Create the estoimator for the AdaBoost Classifier as a local variable \n",
    "\t\tclosed_hype_clf = RandomForestClassifier(\n",
    "\t\t\t\t\t\t\t\t\tn_estimators=self.clf_estimator['n_estimators'], \n",
    "\t\t\t\t\t\t\t\t\tcriterion=self.clf_estimator['criterion'],\n",
    "\t\t\t\t\t\t\t\t\tmax_features=self.clf_estimator['max_features'], \n",
    "\t\t\t\t\t\t\t\t\tbootstrap=self.clf_estimator['bootstrap'],\n",
    "\t\t\t\t\t\t\t\t\tmin_samples_split=self.clf_estimator['min_samples_split'], \n",
    "\t\t\t\t\t\t\t\t\tmin_samples_leaf=self.clf_estimator['min_samples_leaf'], \n",
    "\t\t\t\t\t\t\t\t\tn_jobs=-1\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\n",
    "\t\t### Create the AdaBoost Classifier as a local variable with the given parameters from the current_svm_test dictionary\n",
    "\t\tclosed_clf = AdaBoostClassifier(\n",
    "\t\t\t\t\t\t\tclosed_hype_clf,\n",
    "\t\t\t\t\t\t\tn_estimators=self.clf_params['n_estimators'],\n",
    "\t\t\t\t\t\t\tlearning_rate=self.clf_params['learning_rate'],\n",
    "\t\t\t\t\t\t\talgorithm=self.clf_params['algorithim'],\n",
    "\t\t\t\t\t\t\trandom_state=1)\n",
    "\t\t\n",
    "\t\t### Fit the model to the training data\n",
    "\t\tclosed_clf.fit(X_train, y_train)\n",
    "\n",
    "\t\t### Predict the outcome of the test data\n",
    "\t\tself.closed_pred \t\t= closed_clf.predict(X_test)\n",
    "\n",
    "\t\t### Calculate the accuracy, f1 score, false negative rate, recall, precision, and specificity of the model\n",
    "\t\tself.closed_accuracy \t= accuracy_score(y_test, self.closed_pred)\n",
    "\t\tself.closed_f1 \t\t\t= f1_score(y_test, self.closed_pred)\n",
    "\t\tself.closed_fnr, \n",
    "\t\tself.closed_recall, \n",
    "\t\tself.closed_precision, \n",
    "\t\tself.closed_specificity\t= ratios(y_test, self.closed_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Serialisable\n",
    "This simple helper function takes an entity as a paramater and returns it, after having been converted to a JSON serialisable type. Given the only abstract type being used is a **Numpy Int32**, there only needs to be one conditional. Every other type can be converted at base value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_serialisable(ent):\n",
    "    if isinstance(ent, np.int32):\n",
    "        return int(ent)\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write JSON\n",
    "\n",
    "While it would have been entirely possible to use the the classes default __dict__ function, this gives little control over the how the results are displayed: decreasing readabaility. It is true, given the JSON file extnesion, the data could simply be extracted and accessed in a more appropriate way. But it made more sense to provide a file that is also legible to a human aswel. This means that the user could quickly take a look at the results manually, without having to decypher the cryptic positioning of the variables within the dictionary.\n",
    "\n",
    "Additionally, it would have also been possible to store the array as an array. However, given the length of the array: it seemed more logical to include it as a string, creating a row instead of a 100 line column. This also makes direct comparison between predictions and expected results much easier as they run in paralel.\n",
    "\n",
    "#### Paramaters:\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>class_name</strong> \t\t: This paramater should either be an <strong>svm_model</strong> or <strong>ada_model</strong>. \n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>output_file</strong> \t\t: This paramater should be of type <strong>string</strong> and <em>finish with a \".json\" file extension</em>. It will not create an error if a txt is used, but some json functionality could be limited.\n",
    "\t</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(class_name, output_file):\n",
    "\t\n",
    "\t## Create a dictionary to store the class variables and the results of the model\n",
    "\tclass_dict = {\n",
    "\t\t\"features\": {# Dictionary created empty, to be filled with the features of corresponding model.\n",
    "\t\t},\n",
    "\t\t\"results\": { # Dictionary used to store the metrics of the model \n",
    "\t\t\t\"accuracy\": class_name.closed_accuracy,#float\n",
    "\t\t\t\"f1\": class_name.closed_f1,#float\n",
    "\t\t\t\"fnr\": class_name.closed_fnr,#float\n",
    "\t\t\t\"recall\": class_name.closed_recall,#float\n",
    "\t\t\t\"precision\": class_name.closed_precision,#float\n",
    "\t\t\t\"specificity\": class_name.closed_specificity#float\n",
    "\t\t},\n",
    "\t\t\"predictions\": { # Dictionary used to store the predictions of the model\n",
    "\t\t\t\"y_pred\": str(class_name.closed_pred.tolist()),#array as string\n",
    "\t\t\t\"y_true\": str(y_test.tolist())#array as string \n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\t## Check if the class_name is an instance of the svm_model class\n",
    "\tif(isinstance(class_name, svm_model)):\n",
    "\t\tclass_dict[\"features\"] = { # If the class_name is an instance of the svm_model class, fill the features dictionary with the features of the model\n",
    "\t\t\t\"kernel\": str(class_name.kernel),#str\n",
    "\t\t\t\"max_iter\": class_name.max_iter,#int\n",
    "\t\t\t\"decision_function_shape\": str(class_name.func_shape),#str\n",
    "\t\t\t\"probability\": class_name.probability,#bool\n",
    "\t\t\t\"shrinking\": class_name.shrinking,#bool\n",
    "\t\t\t\"tollerance\": class_name.tollerance,#float\n",
    "\t\t\t\"C\": class_name.C#float\n",
    "\t\t}\n",
    "\telif(isinstance(class_name, ada_model)):\n",
    "\t\tclass_dict[\"features\"] = { # If the class_name is an instance of the ada_model class, fill the features dictionary with the features of the model\n",
    "\t\t\t\"Estimator\": {\n",
    "\t\t\t\t\"n_estimators\": class_name.clf_estimator['n_estimators'],#int\n",
    "\t\t\t\t\"criterion\": str(class_name.clf_estimator['criterion']),#str\n",
    "\t\t\t\t\"max_features\": str(class_name.clf_estimator['max_features']),#str\n",
    "\t\t\t\t\"bootstrap\": class_name.clf_estimator['bootstrap'],#bool\n",
    "\t\t\t\t\"min_samples_split\": class_name.clf_estimator['min_samples_split'],#int\n",
    "\t\t\t\t\"min_samples_leaf\": class_name.clf_estimator['min_samples_leaf']#int\n",
    "\t\t\t},\n",
    "\t\t\t\"Params\": {\n",
    "\t\t\t\t\"n_estimators\": class_name.clf_params['n_estimators'],#int\n",
    "\t\t\t\t\"learning_rate\": class_name.clf_params['learning_rate'],#float\n",
    "\t\t\t\t\"algorithim\": str(class_name.clf_params['algorithim'])#str\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\n",
    "\t_tojson_ = json.dumps(class_dict, default=convert_to_serialisable, indent=4) # Convert the class_dict to a JSON string\n",
    "\n",
    "\twith open(output_file, 'a') as f: # Open the output file in append mode and write the JSON string to the file\n",
    "\n",
    "\t\tf.write(str(_tojson_ ) + ',\\n') # Add a comma and a newline character to the end of the JSON string to separate the different tests\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Comparison\n",
    "\n",
    "This is quite a simple function. It checks that **INSERTMETRICNAME** is higher than 70%, then proceeds to compare the current test against the current Top 10 solutions found. If a new best solution is found, the **solution_list** and **accuracy_list** are appended respectivly. The solution is then written to the correctly named JSON file. If the length of the list exceeds 10 items, the item at the front of the list is removed. \n",
    "\n",
    "#### Paramaters:\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>current_test</strong> \t: This paramater should either be an <strong>svm_model</strong> or <strong>ada_model</strong>. \n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>solution_name</strong> \t: This paramater should be of type <strong>string</strong> and <em>finish with a \".json\" file extension</em>. It will not create an error if a txt is used, but some json functionality could be limited.\n",
    "\t</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_worst(current_test, solution_name):\n",
    "\n",
    "\t## for each solution in the solution list, where i stores the current solution being compared to the current test\n",
    "\tfor i in range(len(solution_list)):\n",
    "\n",
    "\t\t## cbeck if the current test's preformance metric is more optimal than the current solution's preformance metric\n",
    "\t\tif current_test.closed_fnr < accuracy_list[i]:\n",
    "\t\t\t## append the current test to the solution list and the accuracy of the current test to the accuracy list\n",
    "\t\t\tsolution_list.append(current_test)\n",
    "\t\t\taccuracy_list.append(current_test.closed_fnr)\n",
    "\n",
    "\t\t\t## write the current test to the output file\n",
    "\t\t\twrite_json(current_test, solution_name)\n",
    "\n",
    "\n",
    "\t\t\t## check if the length of the solution list is greater than 10\n",
    "\t\t\tif(len(solution_list) > 10):\n",
    "\t\t\t\t## pop the first element from the solution list and the accuracy list\n",
    "\t\t\t\tsolution_list.pop(0)\n",
    "\t\t\t\taccuracy_list.pop(0)\n",
    "\n",
    "\t\t\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_best(current_test, solution_name):\n",
    "\n",
    "\tfor i in range(len(solution_list)):\n",
    "\n",
    "\t\tif current_test.closed_fnr > accuracy_list[i]:\n",
    "\t\t\tsolution_list.append(current_test)\n",
    "\t\t\taccuracy_list.append(current_test.closed_fnr)\n",
    "\n",
    "\t\t\twrite_json(current_test, solution_name)\n",
    "\n",
    "\t\t\tif(len(solution_list) > 10):\n",
    "\t\t\t\tsolution_list.pop(0)\n",
    "\t\t\t\taccuracy_list.pop(0)\n",
    "\n",
    "\t\t\treturn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systematic Grid Search\n",
    "\n",
    "**SKLearn** deos include a standard grid search function: *GridSearchCV*. It is described as an \"*exhaustive* search\" over a definied paramater list. First it trains each possibility of **hyperparamaters** and scores them, using the most admirable for the final predictions. This function is ***increably exhaustive*** when searching over a large dataset. Instead of preforming an optimised search, this algorithm aims to save all possibilities in memory. This would be insignifigant when only checking a few hundred tests. However, with the hundreds of millions of combinations: **how do you know which to test**? \n",
    "\n",
    "It is possible to use the GridSearchCV function for these millions of test paramaters, however the computational requirements are *linear to the **number of tests** $*$ **memory usage for the model selected***. This can easily cause any computer to come to a grinding hault and freeze, even possibly resulting in a BSOD error. The *optimal memory usage should be **memory usage for the model selected** $*$ **number of best solutions***. For this search, as seen in the *Comparison Function*, the max number of best solitions to be stored in active memory should not exceed the number of best solutions, *10*, and an additional solution for the one being tested. This gives a *final memory usage of **11** $*$ **memory usage for the model selected***.\n",
    "\n",
    "Given the memory optimisation of the custom search functions, it would be possible to preform searches on computers with limited memory availability. Dependent on the speed of the computer, and the size of the test, this could still take days to analyse for the most optimal hyperparamaters: however, in theory it should only need to run once as all best solutions found are stored in an external data-structure. **More will be spoken about that later**, but the key benefits of searching in this style is that it gives *functionalty to find the best paramaters inside of an **infinitly large pool of paramaters***. That is, ***so long as you have enough time to wait***. \n",
    "\n",
    "This is a deal breaker for the health industry, whos computer power usually lags behind civilisation due to lack of funding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Functions\n",
    "\n",
    "Both of these functions are structured in the same way for simplistic calling conventions, they also use the same logic: giving exceptions for differences needed by the individual model.\n",
    "\n",
    "Whent he function is called, a few new *local varaibles* are defined. **iterationCount** is inconsequential to the flow of the actual search, however does it allows the program operator to see how many tests have been completed. This is used in combination with the **number_of_tests** constant, which is calculated by multiplying the length of all of the hyper paramater arrays together. These are later used to give a direct number of tests remaining, affirming to the operator: *tests are still being executed*. \n",
    "\n",
    "Next, the **Best Solutions** output is prepared. A new file is created in the JSON file format and a singular bracket and newline is written. This allows the different, best solutions, to be indexed and referenced as independent dictionarys. The final stage of preperation occurs when pushing an empty solution and the worst possible score for the model to acheive. This kick-starts the solution comparison, without it there is nothing to compare against and nothing to iterate. Because this data is added to the respective lists directly, there they will not be outputted to the **Best Solutions**. Neither will they be output to the **Best 10 Solutions**, the first few tests will overwite them no *matter how good they actually are*: the initial push is relativly much worse.\n",
    "\n",
    "Finally, the bullk of the functions. Both are rooted on the principle: multiple nested for loops, incrementally cycling through the test paramaters held by **data**. These values are passed to the relative model's dictionary which is then subsequently passed to the class instance reprenting the Machine Learning model chosen. The prediction function is then called, on termination the class is passed to the *Comparison Function*. \n",
    "\n",
    "On completion of each itteration, an addition macro is applied to the **iterationCount** variable so that the completion percentage of the search can be accuratly seen. Every time the algorithm completes one inner loop: the **Iterations** TXT is overwritten.\n",
    "\n",
    "Once the entire search as been completed, a final closing bracket is added to the **Best Solutions**. Finally, the **Best 10 Solutions** are produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramaters:\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>data</strong> \t: This paramater should be of type Dictionary and should hold the <strong>svm_tests</strong>, or <strong>ada_ensambles_tests</strong>. \n",
    "\t</p>\n",
    "</div>\n",
    "\n",
    "### Outputs:\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>Best Results</strong> \t: This output will be in the JSON file format and <em>will be generated, <strong>only  on completion of the entire search</strong></em>.\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>Results</strong> \t: This output will be in the JSON file format and <em>will be appended <strong>every time a new best solution is found</strong></em>.\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>Iterations</strong> \t: This output will be in the TXT file format and <em>will be overwirtten <strong>every few hundred test solutions</strong></em>.\n",
    "\t</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform a depth first search on the SVM model\n",
    "def svmDepthFirstSearch(data):\n",
    "\n",
    "\t## Clear the solution list and the accuracy list\n",
    "\tsolution_list.clear()\n",
    "\taccuracy_list.clear()\n",
    "\n",
    "\t## Create a variable to store the number of tests that have beeb preformed\n",
    "\titerationCount = 0\n",
    "\n",
    "\t## Calculate the number of tests that will be preformed\n",
    "\tnumber_of_tests = (\n",
    "\t\tlen(data['kernel']) *\n",
    "\t\tlen(data['decision_function_shape']) *\n",
    "\t\tlen(data['possibility']) *\n",
    "\t\tlen(data['possibility']) *\n",
    "\t\tlen(data['max_iter']) *\n",
    "\t\tlen(data['tollerance']) *\n",
    "\t\tlen(data['C'])\n",
    "\t)\n",
    "\n",
    "\n",
    "\t## Setupt the svm_results.json file to store the best solutions discovered by the tests\n",
    "\twith open(\"svm_results.json\", 'w') as f:\n",
    "\t\t## write the opening square bracket to the file to start the JSON array \n",
    "\t\tf.write(\"[\\n\")\n",
    "\t\n",
    "\n",
    "\t## Append an empty object to the solution list and set the accuracy to the worst possible metric score\n",
    "\tsolution_list.append(current_svm_data)\n",
    "\taccuracy_list.append(1.0)\n",
    "\n",
    "\t## Itterate through the different parameters in the data dictionary\n",
    "\tfor kernel in data['kernel']:\n",
    "\t\tfor function_shape in data['decision_function_shape']:\n",
    "\t\t\tfor probability in data['possibility']:\n",
    "\t\t\t\tfor shrinking in data['possibility']:\n",
    "\t\t\t\t\tfor max_iter in data['max_iter']:\n",
    "\t\t\t\t\t\tfor tol in data['tollerance']:\n",
    "\t\t\t\t\t\t\tfor C in data['C']:\n",
    "\n",
    "\t\t\t\t\t\t\t\t## Set the current_svm_data dictionary to the current parameters indexed by the loop\n",
    "\t\t\t\t\t\t\t\tcurrent_svm_data['kernel'] = kernel\n",
    "\t\t\t\t\t\t\t\tcurrent_svm_data['decision_function_shape'] = function_shape\n",
    "\t\t\t\t\t\t\t\tcurrent_svm_data['probability'] = probability\n",
    "\t\t\t\t\t\t\t\tcurrent_svm_data['shrinking'] = shrinking\n",
    "\t\t\t\t\t\t\t\tcurrent_svm_data['max_iter'] = max_iter\n",
    "\t\t\t\t\t\t\t\tcurrent_svm_data['tollerance'] = tol\n",
    "\t\t\t\t\t\t\t\tcurrent_svm_data['C'] = C\n",
    "\n",
    "\n",
    "\t\t\t\t\t\t\t\t## Create a new instance of the svm_model class with the current_svm_data dictionary\n",
    "\t\t\t\t\t\t\t\tcurrent_test = svm_model(current_svm_data)\n",
    "\n",
    "\t\t\t\t\t\t\t\t## Call the predict method of the current_test instance\n",
    "\t\t\t\t\t\t\t\tcurrent_test.predict()\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t## Preform a comparison check\n",
    "\t\t\t\t\t\t\t\tcheck_worst(current_test, \"svm_results.json\")\n",
    "\n",
    "\t\t\t\t\t\t\t\t## Increment the iteration count\n",
    "\t\t\t\t\t\t\t\titerationCount += 1\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t## Write the current iteration count to the file to keep track of the progress EVERY TIME THE TOLLERANCE IS CHANGED\n",
    "\t\t\t\t\t\t\twith open(\"svm_iterations.txt\", 'w') as f:\n",
    "\t\t\t\t\t\t\t\tf.write(\"Iteration: \" + str(iterationCount) + \"/\" + str(number_of_tests))\n",
    "\n",
    "\n",
    "\t## Write the closing square bracket to the file to end the JSON array\n",
    "\twith open(\"svm_results.json\", 'a') as f:\n",
    "\t\tf.write(\"]\\n\")\n",
    "\n",
    "\n",
    "\t## Write the best solutions to the output file\n",
    "\t\t\n",
    "\twith open(\"svm_best_results.json\", 'w') as f:\n",
    "\t\tf.write(\"[\\n\")\n",
    "\n",
    "\tfor i in range(len(solution_list)):\n",
    "\t\twrite_json(solution_list[i], \"svm_best_results.json\")\n",
    "\n",
    "\twith open(\"svm_best_results.json\", 'a') as f:\n",
    "\t\tf.write(\"]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaDepthSearch(data):\n",
    "\n",
    "\n",
    "\t## Clear the solution list and the accuracy list\n",
    "\tsolution_list.clear()\n",
    "\taccuracy_list.clear()\n",
    "\n",
    "\t## Create a variable to store the number of tests that have been preformed\n",
    "\titerationCount = 0\n",
    "\n",
    "\t## Calculate the number of tests which will be preformed\n",
    "\tnumber_of_tests = (\n",
    "\t\tlen(data['Estimator']['n_estimators']) *\n",
    "\t\tlen(data['Estimator']['criterion']) *\n",
    "\t\tlen(data['Estimator']['max_features']) *\n",
    "\t\tlen(data['Estimator']['bootstrap']) *\n",
    "\t\tlen(data['Estimator']['min_samples_split']) *\n",
    "\t\tlen(data['Estimator']['min_samples_leaf']) *\n",
    "\t\tlen(data['Params']['n_estimators']) *\n",
    "\t\tlen(data['Params']['learning_rate']) *\n",
    "\t\tlen(data['Params']['algorithim'])\n",
    "\t)\n",
    "\n",
    "\t## Set up the ada_results.json file to store the best solutions discovered by the tests\n",
    "\twith open(\"ada_results.json\", 'w') as f:\n",
    "\n",
    "\t\t## write the opening square bracket to the file to start the JSON array\n",
    "\t\tf.write(\"[\\n\")\n",
    "\t\t\n",
    "\n",
    "\t## Append an empty object to the solution list\n",
    "\tsolution_list.append(current_ada_data)\n",
    "\t## Set the accuracy to the worst possible metric score\n",
    "\taccuracy_list.append(0.0)\n",
    "\n",
    "\n",
    "\t## Itterate through the different parameters in the data dictionary relating to the Random Forest Classifier\n",
    "\tfor n_estimators in data['Estimator']['n_estimators']:\n",
    "\t\tfor criterion in data['Estimator']['criterion']:\n",
    "\t\t\tfor max_features in data['Estimator']['max_features']:\n",
    "\t\t\t\tfor bootstrap in data['Estimator']['bootstrap']:\n",
    "\t\t\t\t\tfor min_samples_split in data['Estimator']['min_samples_split']:\n",
    "\t\t\t\t\t\tfor min_samples_leaf in data['Estimator']['min_samples_leaf']:\n",
    "\t\t\t\t\t\t\t\n",
    "\n",
    "\t\t\t\t\t\t\t## Set the estimator dictionary to the current parameters indexed by the loop\n",
    "\t\t\t\t\t\t\tcurrent_ada_data['Estimator']['n_estimators'] = n_estimators\n",
    "\t\t\t\t\t\t\tcurrent_ada_data['Estimator']['criterion'] = criterion\n",
    "\t\t\t\t\t\t\tcurrent_ada_data['Estimator']['max_features'] = max_features\n",
    "\t\t\t\t\t\t\tcurrent_ada_data['Estimator']['bootstrap'] = bootstrap\n",
    "\t\t\t\t\t\t\tcurrent_ada_data['Estimator']['min_samples_split'] = min_samples_split\n",
    "\t\t\t\t\t\t\tcurrent_ada_data['Estimator']['min_samples_leaf'] = min_samples_leaf\n",
    "\n",
    "\n",
    "\t\t\t\t\t\t\t## Itterate through the different parameters in the data dictionary relating to the AdaBoost Classifier\n",
    "\t\t\t\t\t\t\tfor n_estimators_params in data['Params']['n_estimators']:\n",
    "\t\t\t\t\t\t\t\tfor learning_rate in data['Params']['learning_rate']:\n",
    "\t\t\t\t\t\t\t\t\tfor algorithim in data['Params']['algorithim']:\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t\t\t## Set the hyperparamaters dictionary to the current parameters indexed by the loop\n",
    "\t\t\t\t\t\t\t\t\t\tcurrent_ada_data['Params']['n_estimators'] = n_estimators_params\n",
    "\t\t\t\t\t\t\t\t\t\tcurrent_ada_data['Params']['learning_rate'] = learning_rate\n",
    "\t\t\t\t\t\t\t\t\t\tcurrent_ada_data['Params']['algorithim'] = algorithim\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\t## Create a new instance of the ada_model class with the current_ada_data dictionary\n",
    "\t\t\t\t\t\t\t\t\t\tcurrent_test = ada_model(current_ada_data)\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\t## Call the predict method of the current_test instance\n",
    "\t\t\t\t\t\t\t\t\t\tcurrent_test.predict()\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\t## Preform a comparison check\n",
    "\t\t\t\t\t\t\t\t\t\tcheck_best(current_test, \"ada_results.json\")\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\t## Increment the iteration count\n",
    "\t\t\t\t\t\t\t\t\t\titerationCount += 1\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t\t## Write the current iteration count to the file to keep track of the progress EVERY TIME THE LEARNING_RATE IS CHANGED\n",
    "\t\t\t\t\t\t\t\t\twith open(\"ada_iterations.txt\", 'w') as f:\n",
    "\t\t\t\t\t\t\t\t\t\tf.write(\"Iteration: \" + str(iterationCount) + \"/\" + str(number_of_tests))\n",
    "\n",
    "\n",
    "\t## Write the closing square bracket to the file to end the JSON array\n",
    "\twith open(\"ada_results.json\", 'a') as f:\n",
    "\t\tf.write(\"]\\n\")\n",
    "\n",
    "\n",
    "\t## Write the best solutions to the output file\n",
    "\twith open(\"ada_best_results.json\", 'w') as f:\n",
    "\t\tf.write(\"[\\n\")\n",
    "\n",
    "\tfor i in range(len(solution_list)):\n",
    "\t\twrite_json(solution_list[i], \"ada_best_results.json\")\n",
    "\n",
    "\twith open(\"ada_best_results.json\", 'a') as f:\n",
    "\t\tf.write(\"]\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute the Search Algorithms\n",
    "\n",
    "These functions simply call the aforementioned search algorithms with the predefined list of hyperparamaters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svmDepthFirstSearch(svm_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaDepthSearch(ada_ensambles_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Results into memory\n",
    "\n",
    "This is where the application reads the results of a previously completed search. It is important to note, this function does not load the newly created JSON files. Instead, it reads a finished test stored in a separated folder. This is to ensure that the previously learned solutions are not removed and can be reapplied in the future. Using the **JSON.load** method, its possible to easily translate a json array into a python Dictionary Array. This will make referencing much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the array to store the solutions\n",
    "\n",
    "array_of_solutions = []\n",
    "\n",
    "def read_json(filepath):\n",
    "\t## Open the file in read mode\n",
    "\twith open(filepath, 'r') as f:\n",
    "\t\t## Load the JSON file into the array_of_solutions variable\n",
    "\t\treturn json.load(f)\n",
    "\n",
    "\n",
    "## Read the best results from the SVM model\n",
    "array_of_solutions = read_json(\"TestOutputsSVM\\soltuions320k.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panda Restructure\n",
    "\n",
    "Creating a panda **DataFrame** automatically converts the data into columns and rows, like a standardised table of elements. This completed data structure should be much more readable to the operator and can then be used to make predictions about how the model could preform under the correct hyperparamaters. This then displays the 80 solutions found in the previous test. It may be apparent that the **solutions320k.json** file is formatted differently to the one produced by the grid search. This results datafile was generated before the **write_json** file was designed. This previous method resulted in poor readability and messy datastrcutures. The new method is much more streamlined, but converting the reading method for the svm_results would take some consideration.\n",
    "\n",
    "***@@**: This step is unnecessasary- it just helps to give an idea of the data to be processed, and gives a better understanding of how the proceeding functions handle said data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    kernel func_shape  shrinking  probability  tollerance  max_iter  C  \\\n",
      "0     poly        ovo       True         True      0.0002      1000  7   \n",
      "1     poly        ovo       True         True      0.0003      1000  7   \n",
      "2     poly        ovo       True         True      0.0004      1000  7   \n",
      "3     poly        ovo       True         True      0.0005      1000  7   \n",
      "4     poly        ovo       True         True      0.0006      1000  7   \n",
      "..     ...        ...        ...          ...         ...       ... ..   \n",
      "75  linear        ovo       True         True      0.0006      1303  7   \n",
      "76  linear        ovo       True         True      0.0007      1303  7   \n",
      "77  linear        ovo       True         True      0.0008      1303  7   \n",
      "78  linear        ovo       True         True      0.0009      1303  7   \n",
      "79  linear        ovo       True         True      0.0010      1303  7   \n",
      "\n",
      "    closed_accuracy  closed_precision  closed_recall  closed_f1  closed_fnr  \\\n",
      "0          0.740260          0.647059       0.600000   0.622642    0.400000   \n",
      "1          0.740260          0.647059       0.600000   0.622642    0.400000   \n",
      "2          0.740260          0.647059       0.600000   0.622642    0.400000   \n",
      "3          0.740260          0.647059       0.600000   0.622642    0.400000   \n",
      "4          0.740260          0.647059       0.600000   0.622642    0.400000   \n",
      "..              ...               ...            ...        ...         ...   \n",
      "75         0.772727          0.656250       0.763636   0.705882    0.236364   \n",
      "76         0.772727          0.656250       0.763636   0.705882    0.236364   \n",
      "77         0.772727          0.656250       0.763636   0.705882    0.236364   \n",
      "78         0.772727          0.656250       0.763636   0.705882    0.236364   \n",
      "79         0.772727          0.656250       0.763636   0.705882    0.236364   \n",
      "\n",
      "                                          closed_pred  \n",
      "0   [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, ...  \n",
      "1   [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, ...  \n",
      "2   [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, ...  \n",
      "3   [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, ...  \n",
      "4   [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, ...  \n",
      "..                                                ...  \n",
      "75  [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...  \n",
      "76  [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...  \n",
      "77  [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...  \n",
      "78  [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...  \n",
      "79  [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...  \n",
      "\n",
      "[80 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "#frame the data into a pandas dataframe\n",
    "df = pd.DataFrame(array_of_solutions)\n",
    "\n",
    "#display the data\n",
    "print(df[0:80])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Helper Functions \n",
    "\n",
    "These functions, while simple in nature, allow for far cleaner code: overall, improving readability. Instead of needing to write these code blocks in-line, presenting them inside functions splits the code. This in tern helps to lessen the code's indentation, negating confusion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Results to Grid* Function\n",
    "\n",
    "These functions are designed to take the results from **read_json**, translating them into a dictionary with arrays corrisoponding to unique values for each paramater. \n",
    "\n",
    "They start by defining the dictionary for which to store the unique values, then itterating over each individaual entity in the results file. During itteration, each value is added to a list. Once this cycle has completed, a new dictionary is defined with the same keys. Lists are then converted into sets, removing any non-unique values, and finally converted back into lists for integrity. This final dictionary, filled with keys assosiated to unique lists is returned for later use. \n",
    "\n",
    "It should be worth noting that the outputs will **not** include the metrics produced by the model. These will not be needed for the proceeding functions. \n",
    "\n",
    "#### Paramaters:\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>non_unique_values</strong> \t: This paramater is an array of dictionarys produced by the **read_json()** function. \n",
    "\t</p>\n",
    "</div>\n",
    "\n",
    "#### Returns:\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>unique_dict</strong> \t: This variable is a Dictionary which stores all unique *best* inputs assosiated with the respected model.\n",
    "\t</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_results_to_grid(non_unique_values):\n",
    "\n",
    "\t## Create a dictionary to store the unique values of the non_unique_values dictionary\n",
    "\tunique_search = {\n",
    "\t\t'kernel': [],\n",
    "\t\t'max_iter': [],\n",
    "\t\t'decision_function_shape': [],\n",
    "\t\t'probability': [],\n",
    "\t\t'shrinking': [],\n",
    "\t\t'C': [],\n",
    "\t\t'tollerance': [],\n",
    "\t}\n",
    "\n",
    "\t## Itterate through the non_unique_values dictionary\n",
    "\tfor i in range(len(non_unique_values)):\n",
    "\t\t## Append the values to the unique_search dictionary\n",
    "\t\tunique_search['kernel'].append(non_unique_values[i]['kernel'])\n",
    "\t\tunique_search['max_iter'].append(non_unique_values[i]['max_iter'])\n",
    "\t\tunique_search['decision_function_shape'].append(non_unique_values[i]['func_shape'])\n",
    "\t\tunique_search['probability'].append(non_unique_values[i]['probability'])\n",
    "\t\tunique_search['shrinking'].append(non_unique_values[i]['shrinking'])\n",
    "\t\tunique_search['C'].append(non_unique_values[i]['C'])\n",
    "\t\tunique_search['tollerance'].append(non_unique_values[i]['tollerance'])\n",
    "\n",
    "\t## Create a dictionary to store the unique values of the unique_search dictionary\n",
    "\tunique_dict = {\n",
    "\t\t## Cast the lists to sets to remove duplicates and then cast them back to lists\n",
    "\t\t'kernel': list(set(unique_search['kernel'])),\n",
    "\t\t'max_iter': list(set(unique_search['max_iter'])),\n",
    "\t\t'decision_function_shape': list(set(unique_search['decision_function_shape'])),\n",
    "\t\t'probability': list(set(unique_search['probability'])),\n",
    "\t\t'shrinking': list(set(unique_search['shrinking'])),\n",
    "\t\t'C': list(set(unique_search['C'])),\n",
    "\t\t'tollerance': list(set(unique_search['tollerance'])),\n",
    "\t}\n",
    "\n",
    "\n",
    "\treturn unique_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada_results_to_grid(non_unique_values):\n",
    "\tunique_search = {\n",
    "\t\t'estimator': {\n",
    "\t\t\t\t\"n_estimators\": [],#int\n",
    "\t\t\t\t\"criterion\": [],#str\n",
    "\t\t\t\t\"max_features\": [],#str\n",
    "\t\t\t\t\"bootstrap\": [],#bool\n",
    "\t\t\t\t\"min_samples_split\": [],#int\n",
    "\t\t\t\t\"min_samples_leaf\": []#int\n",
    "\t\t\t},\n",
    "\t\t'params': {\n",
    "\t\t\t\t\"n_estimators\": [],#int\n",
    "\t\t\t\t\"learning_rate\": [],#float\n",
    "\t\t\t\t\"algorithim\": []#str\n",
    "\t\t\t}\n",
    "\t}\n",
    "\n",
    "\tfor i in range(len(non_unique_values)):\n",
    "\t\tunique_search['estimator']['n_estimators'].append(non_unique_values[i]['Estimator']['n_estimators'])\n",
    "\t\tunique_search['estimator']['criterion'].append(non_unique_values[i]['Estimator']['criterion'])\n",
    "\t\tunique_search['estimator']['max_features'].append(non_unique_values[i]['Estimator']['max_features'])\n",
    "\t\tunique_search['estimator']['bootstrap'].append(non_unique_values[i]['Estimator']['bootstrap'])\n",
    "\t\tunique_search['estimator']['min_samples_split'].append(non_unique_values[i]['Estimator']['min_samples_split'])\n",
    "\t\tunique_search['estimator']['min_samples_leaf'].append(non_unique_values[i]['Estimator']['min_samples_leaf'])\n",
    "\n",
    "\t\tunique_search['params']['n_estimators'].append(non_unique_values[i]['Params']['n_estimators'])\n",
    "\t\tunique_search['params']['learning_rate'].append(non_unique_values[i]['Params']['learning_rate'])\n",
    "\t\tunique_search['params']['algorithim'].append(non_unique_values[i]['Params']['algorithim'])\n",
    "\n",
    "\t\n",
    "\tunique_dict = {\n",
    "\t\t'estimator': {\n",
    "\t\t\t\t\"n_estimators\": list(set(unique_search['estimator']['n_estimators'])),#int\n",
    "\t\t\t\t\"criterion\": list(set(unique_search['estimator']['criterion'])),#str\n",
    "\t\t\t\t\"max_features\": list(set(unique_search['estimator']['max_features'])),#str\n",
    "\t\t\t\t\"bootstrap\": list(set(unique_search['estimator']['bootstrap'])),#bool\n",
    "\t\t\t\t\"min_samples_split\": list(set(unique_search['estimator']['min_samples_split'])),#int\n",
    "\t\t\t\t\"min_samples_leaf\": list(set(unique_search['estimator']['min_samples_leaf']))#int\n",
    "\t\t\t},\n",
    "\t\t'params': {\n",
    "\t\t\t\t\"n_estimators\": list(set(unique_search['params']['n_estimators'])),#int\n",
    "\t\t\t\t\"learning_rate\": list(set(unique_search['params']['learning_rate'])),#float\n",
    "\t\t\t\t\"algorithim\": list(set(unique_search['params']['algorithim']))#str\n",
    "\t\t\t}\n",
    "\t}\n",
    "\n",
    "\treturn unique_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small example code\n",
    "\n",
    "This example of how functions can be used, in conjunction, is desinged to quicly show the operator the absolute best values for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "        \"kernel\": [\n",
      "                \"poly\",\n",
      "                \"linear\"\n",
      "        ],\n",
      "        \"max_iter\": [\n",
      "                1121,\n",
      "                1060,\n",
      "                1000,\n",
      "                1161,\n",
      "                1141,\n",
      "                1303,\n",
      "                1080,\n",
      "                1020\n",
      "        ],\n",
      "        \"decision_function_shape\": [\n",
      "                \"ovo\"\n",
      "        ],\n",
      "        \"probability\": [\n",
      "                true\n",
      "        ],\n",
      "        \"shrinking\": [\n",
      "                false,\n",
      "                true\n",
      "        ],\n",
      "        \"C\": [\n",
      "                7\n",
      "        ],\n",
      "        \"tollerance\": [\n",
      "                0.0009000000000000001,\n",
      "                0.0004,\n",
      "                0.0008,\n",
      "                0.00030000000000000003,\n",
      "                0.0002,\n",
      "                0.0007000000000000001,\n",
      "                0.0011,\n",
      "                0.0006000000000000001,\n",
      "                0.0001,\n",
      "                0.001,\n",
      "                0.0005\n",
      "        ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(svm_results_to_grid(read_json(\"TestOutputsSVM\\soltuions320k.json\")), indent=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Optimised Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_SK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
