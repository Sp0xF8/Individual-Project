{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "<h3> These imports are shared across the whole application and not specific to either model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json as json\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SVM Imports</h3>\n",
    "<p>These imports are specifically related to the SVM's functionality</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ADA Ensambles</h3>\n",
    "<p>These imports are specifically related to the ADA models</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Data ready for interperating\n",
    "<p>In this stage we are using the Pandas libary to load the CSV data file.</p>\n",
    "\t<div style=\"margin-left: 20px;\">\n",
    "\t\t<p>- This helps by giving us functionality to use a wide array of methods</p>\n",
    "\t</div>\n",
    "\t<p>The data is then printed to allow for easy refencing and understanding of base data</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the CSV file\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "print(data[:10])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importance of Splitting Test and Training Data in Machine Learning Models</h3>\n",
    "\n",
    "In Machine Learning, it is essential to split the dataset into training and test sets.\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>The importance of having a <strong> Validation Dataset </strong> is to have a selection of data for which the learned model will be scored against. By knowing the result of said inputs, it is possible to match them against the predictions made by the model. This is the most accurate way to test. A counter-option would be to test against the trained data, however this wouldn't be a real-world example of predicting new labels. Knowing the success rate of predicted models means the model can be adjusted until the preformance is satisfactory. \n",
    "\t</p>\n",
    "</div>\n",
    "Additionally, the from the training <strong>data</strong> provided by the CSV, the outcome must be dropped for the list of inputs (<strong>X</strong>). Everything, besides the outcome, should also be dropped from the list of outputs (<strong>y</strong>).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the initial X and y lists from the data CSV file\n",
    "X = data.drop(\"Outcome\", axis=1)\n",
    "y = data[\"Outcome\"]\n",
    "\n",
    "\n",
    "#split the data into training and testing data, 80% training and 20% testing- random state is set to 42 because it is the answer to everything\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Variables\n",
    "\n",
    "\n",
    "<h3>Creating SVM Variables</h3>\n",
    "These two variables are of the type Dictonary, which is similar in format to the <strong>Json</strong> file extension. Advantages of setting the models to work in this behavoiur include ease of access and increased readability. It also gives the ability to store mulitple different datatypes in one element and easily export it to a Json file.\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>svm_tests</strong> \t\t\t: This Dictonary is used to store the range of hyper-paramaters passed to the depth first grid search.\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>current_svm_data</strong> \t: This Dictionary is used to store the current hyper-paramaters being passed to the <strong>svm_model</strong> class.\n",
    "\t</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the tests which will be preformed on the SVM\n",
    "svm_tests = {\n",
    "\t'C': np.linspace(1, 100, 100),\n",
    "\t'tollerance': np.linspace(0.0001, 0.1, 100),\n",
    "\t'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "\t'max_iter': np.linspace(10, 10000, 100).astype(int),\n",
    "\t'decision_function_shape': ['ovo', 'ovr'],\n",
    "\t'possibility': [True, False],\n",
    "}\n",
    "\n",
    "# Dictionary to store the current test's data for the SVM\n",
    "current_svm_data = {\n",
    "\t'kernel': None,#\n",
    "\t'max_iter': None,\n",
    "\t'decision_function_shape': None,#\n",
    "\t'probability': None,#\n",
    "\t'shrinking': None,#\n",
    "\t'C': None,\n",
    "\t'tollerance':None#\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating Ensambles Variables</h3>\n",
    "\n",
    "\n",
    "Simiarly to the previous definitions, these variables are also of the type Dictionary. They are, instead, however multi-layered. \n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>ada_ensambles_tests</strong> : This Dictonary is used to store the range of hyper-paramaters passed to the depth first grid search.\n",
    "\t\t<div style=\"margin-left: 50px;\">\n",
    "\t\t\t<p>\n",
    "\t\t\t\t<strong>Estimator</strong> \t\t\t: This Dictonary is used to store the range of hyper-paramaters passed to the <strong>Random Forest Classifier</strong>.\n",
    "\t\t\t</p>\n",
    "\t\t\t<p>\n",
    "\t\t\t\t<strong>Params</strong> \t\t\t: This Dictionary is used to store the range of hyper-paramaters passed to the <strong>Ada Boost classifier</strong>.\n",
    "\t\t\t</p>\n",
    "\t\t</div>\n",
    "\t</p>\n",
    "\t<hr style=\"width:75%;\" align=\"left\">\n",
    "\t<p>\n",
    "\t\t<strong>current_ada_data</strong> \t: This Dictionary is used to store the current hyper-paramaters being passed to the <strong>ada_model</strong> class.\n",
    "\t\t<div style=\"margin-left: 50px;\">\n",
    "\t\t\t<p>\n",
    "\t\t\t\t<strong>Estimator</strong> \t\t\t: This Dictonary is used to store the current hyper-paramaters being passed to the <strong>Random Forest Classifier</strong>.\n",
    "\t\t\t</p>\n",
    "\t\t\t<p>\n",
    "\t\t\t\t<strong>Params</strong> \t\t\t: This Dictionary is used to store the current hyper-paramaters being passed to the <strong>Ada Boost classifier</strong>.\n",
    "\t\t\t</p>\n",
    "\t\t</div>\n",
    "\t</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the tests which will be preformed on the AdaBoost Classifier and Random Forest Classifier\n",
    "ada_ensambles_tests = {\n",
    "\t'Estimator': {\n",
    "\t\t'n_estimators': np.linspace(1, 200, 10).astype(int),\n",
    "\t\t'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "\t\t'max_features': ['sqrt', 'log2'],\n",
    "\t\t'bootstrap': [True, False],\n",
    "\t\t'min_samples_split': np.linspace(2, 11, 10).astype(int),\n",
    "\t\t'min_samples_leaf': np.linspace(2, 6, 5).astype(int)\n",
    "\t},\n",
    "\t'Params': {\n",
    "\t\t'n_estimators': np.linspace(1, 100, 10).astype(int),\n",
    "\t\t'learning_rate': np.linspace(0.1, 3, 10),\n",
    "\t\t'algorithim': ['SAMME', 'SAMME.R']\n",
    "\t}\n",
    "}\n",
    "\n",
    "# Dictionary to store the current test's data for the AdaBoost Classifier and Random Forest Classifier\n",
    "current_ada_data = {\n",
    "\t'Estimator': {\n",
    "\t\t'n_estimators': None,\n",
    "\t\t'criterion': None,\n",
    "\t\t'max_features': None,\n",
    "\t\t'bootstrap': None,\n",
    "\t\t'min_samples_split': None,\n",
    "\t\t'min_samples_leaf': None\n",
    "\t},\n",
    "\t'Params': {\n",
    "\t\t'n_estimators': None,\n",
    "\t\t'learning_rate': None,\n",
    "\t\t'algorithim': None\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Defining the test comparison</h3>\n",
    "These two vairables are of type List and are used to store the best solutions found and their respective metrics. This is essneital in preforming a goal orientated depth first search. Without having a comparison, how can you know if you are finding a better solution?\n",
    "\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>solution_list</strong> \t\t: This List stores class objects of the top 10 solutions, allowing for instant referencing once the search has been completed.\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>accuracy_list</strong> \t\t: This List stores the accuracy of the class object at the same offset and is the list used for direct comparison without having to reference the solutions list; slowing down the, already hundreds of thousands, of tests. \n",
    "\t</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_list = []\n",
    "accuracy_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function and Class Definitions\n",
    "\n",
    "<h3>Ratios Function</h3>\n",
    "This function is an easy way of returning multiple metrics in a more efficent way: using far less function calls.\n",
    "\n",
    "<strong>Paramaters:</strong>\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>y_true</strong> \t\t: This variable is of type numpy array and holds the actual true values for the test data (<strong>y_test</strong>)\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>y_pred</strong> \t\t: This variable is also of type numpy array and holds the predicted values for the test data (<strong>y_test</strong>)\n",
    "\t</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funct to test the SVM model without calling different functions, this is to make the code more readable and efficient\n",
    "def ratios(y_true, y_pred):\n",
    "    ## Get the confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    \n",
    "    ## Calculate False Negative Ratio\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    \n",
    "\t## Calculate Recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "\t## Calculate Precision\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "\t## Calculate specificity\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    return fnr, recall, precision, specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Definitions\n",
    "\n",
    "Using a class instead of a standard function is highly effiecent. It allows for better reuseability, and storage, of data in the long term. This is fundamental concept of Object Orientated Programming. By using OOP in this project, it is much easier to rapidly cycle through a grid of hyper paramaters and subsequently output and results gained. This also means that less indents are reuqired to accomplish the same task, resulting in cleaner code which is much easier to understand.\n",
    "\n",
    "By choosing to use a class, there is inherent access to standardised methods: like the constructor. Both the **svm_model** and **ada_model** both use **constructurs** which accept only one paramater. These are the previously defined respective Dictionarys (**current_svm_data**, **current_ada_data**). In this regard, the constructor is used to not only setup the Machine Learning model but also used to save paramaters. These can later be referenced by a different function, printing the best solutions found by the search. \n",
    "\n",
    "Additionally, the **predict** method is shared between the classes. While they both have vastly different features, they return the same metric values. The models themselves are stored in function-local variables, meaning once the predictions have been made: they are destroyed, freeing memory. The metric data is then stored in appropriatly named class-vairables for later use.\n",
    "\n",
    "\n",
    "### Prediction Types:\n",
    "\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>True Positive</strong> \t\t: This type of classification occurs when the model <em><strong>predicts a Positive result</strong>, and the result is <strong>actually Positive</strong></em>.\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>True Negative</strong> \t\t: This type of classification occurs when the model <em><strong>predicts a Negative result</strong>, and the result is <strong>actually Negative</strong></em>.\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>False Positive</strong> \t: This type of classification occurs when the model <em><strong>predicts a Positive result</strong>, and the result is <strong>actually Negative</strong></em>.\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>False Negative</strong> \t: This type of classification occurs when the model <em><strong>predicts a Negative result</strong>, and the result is <strong>actually Positive</strong></em>.\n",
    "\t</p>\n",
    "</div>\n",
    "\n",
    "### Metric Variables:\n",
    "\n",
    "<div style=\"margin-left: 20px;\">\n",
    "\t<p>\n",
    "\t\t<strong>closed_pred</strong> \t\t: This is a variable of type Numpy Array and holds the <em>predicted true values for the test data</em> (<strong>y_pred</strong>)\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>closed_accuracy</strong> \t: This variable is of the type Float and holds the most simple of the metrics. This is simply the number of <em>correctly classafied datapoints to the number of incorrectly classafied datapoints</em>. Having a good overall accuracy is important but, dependant on what application the model is used for, can be misappropriated.\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>closed_precision</strong> \t: This variable is also of type Float and holds a ratio of <em>correctly identified <strong>True Positives</strong> to the total number of positive predictions</em> made by the model. This is useful for analysing the validity of the positivly predicited models. \n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>closed_specificity</strong> : Similar to <strong>closed_precision</strong>, this variable is also Float. Instead, however, it stores the ratio of <em>correctly identified <strong>True Negatives</strong> to the total number of negative predictions</em> made by the model.  \n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>closed_recall</strong> \t\t: This variable type of Float is used for the <em><strong>Recall</strong>, or <strong>Sensitivity</strong>,</em> of the models predictions. This is <em>the number of <strong>True Positive</strong> predictions to the total number of positive instances</em>. Recall makes excellent pairing if <strong>closed_precision</strong> is also a metric of choice. It helps to give more complete info, as precision wil not specify how many instances were missed: only how accurate it is at finding positive predicitions. This could be misleading, as an easy way to always predict people with diabeties, without missing anyone, is to simply say <em>everyone has diabeties</em>. This would <strong><em>never miss a diabetic</strong>, but would <strong>instantly slow down formal diagnosis</strong> because everyone would be referred</em>.\n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>closed_fnr</strong> \t\t: This variable is of type float and holds the <em><strong>False Negative Ratio</strong></em>. This the invserse of <strong>Recall</strong>, however, for transparancy, using this as a clear metric seemed preferable. Often refered to as the <em>miss rate</em>, this metric is highly valued when missed positive instances is critical. When dealing with any form of diagnosis, it is far more important that the model can correctly identify as many <strong>True Positives</strong> as possible. <em>While it is <strong>less than ideal if a non-diabetic is referred</strong>, this would cause the diagnosis system to slow down, it is <strong>far more important that the diabetic is not overlooked</strong> and left without treatment</em>. It is important to note that <strong>this should not be the only metric used for judgement</strong>. A good choice of pairing for this metric would also be <em>Precision or Specificity</em>. Specificity would provide a well rounded overview by providing insight into if the model is simply predicting everyone as diabetic. \n",
    "\t</p>\n",
    "\t<p>\n",
    "\t\t<strong>closed_f1</strong> \t\t\t: Of type Float, this variable stores the <em>harmonic mean of <strong>precision and recall</strong></em>. This ensures <strong>False Positives</strong> and <strong>False Negatives</strong> are accounted for, and is <em>particularly useful when neither have signifigantly different levels of importance</em>. Given the models previously stressed importance of not allowing for False Negatives, but having some leverage on False Positives: F1 scoring metrics are not entirely relavent. \n",
    "\t</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "### svm_model.predict Method\n",
    "\n",
    "Typically, a prediction function would allow for some way of dynamically setting the data to be predicted. However, given the time complexity of the grid searches employed, it was far more efficent to define the test data globally and reference when needed. If this was to be progressed further, the predict function would need to take a multi-dimensional array as the input (representing the data to classify). For the current scenario, it is suited perfectly and offers an optimised solution.\n",
    "\n",
    "Inside the prediction function, it begins by defining a pipline; through which, the data should be processed before reaching the **Support Vector Classifier**. Given the type of model, and the large separation of the data (*e.g pregnancies: 1, glocouse: 168), the data must be pre-processed before it can be accuratly analysed by the **SVC**. Problems can easily arise in the effectiveness of Linear based algorithms, this is where the **StandardScaler** is useful inside the pipline. Instead of predefining the the patient data (**X_train**, **X_test**, ...) after having been scaled, causing disruptions to the other ML model (**ada_model**), the data is scaled on access of the model. This means new test data added will also not need to be scaled, as the pipline will automatically handle any scaling needs. \n",
    "\n",
    "\tclosed_clf.fit(X_train, y_train)\n",
    "\n",
    "This line of code is where the SVC Pipline is called to train the model. The first paramater, **X_train**, is then passed through the pipline. First being scaled, reducing the variance in the source data, then being passed further down the pipline to the SVC model and specified kernel. At its root, the **Standard Scaler** function is designed to help convert values from different formats into a single scale. It is designed to ensure that each feature has a mean of 0 and a unit of standard deviation. This is useful for Kernels such as *RBF*, which assumes that all features are centered around a 0 origin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SVM Model Definition\n",
    "class svm_model:\n",
    "\n",
    "\t## Constructor - Takes in a data dictionary (current_svm_test) and assigns the values to the class variables\n",
    "\tdef __init__(self, data):\n",
    "\t\tself.kernel \t\t\t\t\t= data['kernel']\n",
    "\t\tself.max_iter \t\t\t\t\t= data['max_iter']\n",
    "\t\tself.func_shape \t\t\t\t= data['decision_function_shape']\n",
    "\t\tself.probability \t\t\t\t= data['probability']\n",
    "\t\tself.shrinking \t\t\t\t\t= data['shrinking']\n",
    "\t\tself.tollerance \t\t\t\t= data['tollerance']\n",
    "\t\tself.C \t\t\t\t\t\t\t= data['C']\n",
    "\n",
    "\n",
    "\n",
    "\t## Predict Function - Uses the current_svm_test from the constructor to create a pipeline and predict the outcome of the test data \n",
    "\tdef predict(self):\n",
    "\n",
    "\t\t### Create the pipeline as a local variable\n",
    "\t\tclosed_clf = make_pipeline(\t\n",
    "\t\t\t\t\t\t\t\t\tStandardScaler(), # Standardise the data before training\n",
    "\t\t\t\t\t\t\t\t\tSVC( # Create the SVC model with the given parameters from the current_svm_test dictionary\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t\t\tkernel=self.kernel, \n",
    "\t\t\t\t\t\t\t\t\t\tmax_iter=self.max_iter, \n",
    "\t\t\t\t\t\t\t\t\t\tdecision_function_shape=self.func_shape, \n",
    "\t\t\t\t\t\t\t\t\t\tprobability=self.probability, \n",
    "\t\t\t\t\t\t\t\t\t\tshrinking=self.shrinking,\n",
    "\t\t\t\t\t\t\t\t\t\tC=self.C,\n",
    "\t\t\t\t\t\t\t\t\t\ttol=self.tollerance\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\t\n",
    "\t\t### Fit the model to the training data \n",
    "\t\tclosed_clf.fit(X_train, y_train)\n",
    "\n",
    "\t\t### Predict the outcome of the test data\n",
    "\t\tself.closed_pred \t\t= closed_clf.predict(X_test)\n",
    "\n",
    "\t\t### Calculate the accuracy, f1 score, false negative rate, recall, precision, and specificity of the model\n",
    "\t\tself.closed_accuracy \t= accuracy_score(y_test, self.closed_pred)\n",
    "\t\tself.closed_f1 \t\t\t= f1_score(y_test, self.closed_pred)\n",
    "\t\t\n",
    "\t\tself.closed_fnr, \n",
    "\t\tself.closed_recall, \n",
    "\t\tself.closed_precision, \n",
    "\t\tself.closed_specificity\t= ratios(y_test, self.closed_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ada_model.predict Method\n",
    "\n",
    "Similarly to the svm_model.predict method, this class's uses all of the same training, prediction and testing functions. Its key difference: the first two \"lines\". Instead of setting up a pipline and standardising the data, there is an option to define an estimator, which will be used as later. The chosen estimator (**closed_hype_clf**) for this project is the **RandomForestClassifier**. Known for its *robustness to overfitting and ability to handle multi-dimensional data*, it was a clear choice for this project. Due to its nature of creating multiple decision tree classifiers and averaging their rules, RandomForestClassifer is a *reasonably performance demanding algorithm*. Depending on the dataset, the *paramaters passed must be very finely tuned in order to acheive outstanding results*. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ada_model:\n",
    "\tdef __init__(self, current_test):\n",
    "\n",
    "\t\tself.clf_estimator = current_test['Estimator']\n",
    "\n",
    "\t\tself.clf_params = current_test['Params']\n",
    "\t\t\n",
    "\n",
    "\n",
    "\tdef predict(self):\n",
    "\t\tclosed_hype_clf = RandomForestClassifier(\n",
    "\t\t\t\t\t\t\t\t\tn_estimators=self.clf_estimator['n_estimators'], \n",
    "\t\t\t\t\t\t\t\t\tcriterion=self.clf_estimator['criterion'],\n",
    "\t\t\t\t\t\t\t\t\tmax_features=self.clf_estimator['max_features'], \n",
    "\t\t\t\t\t\t\t\t\tbootstrap=self.clf_estimator['bootstrap'],\n",
    "\t\t\t\t\t\t\t\t\tmin_samples_split=self.clf_estimator['min_samples_split'], \n",
    "\t\t\t\t\t\t\t\t\tmin_samples_leaf=self.clf_estimator['min_samples_leaf'], \n",
    "\t\t\t\t\t\t\t\t\tn_jobs=-1\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\n",
    "\t\t\n",
    "\t\tclosed_clf = AdaBoostClassifier(\n",
    "\t\t\t\t\t\t\tclosed_hype_clf,\n",
    "\t\t\t\t\t\t\tn_estimators=self.clf_params['n_estimators'],\n",
    "\t\t\t\t\t\t\tlearning_rate=self.clf_params['learning_rate'],\n",
    "\t\t\t\t\t\t\talgorithm=self.clf_params['algorithim'],\n",
    "\t\t\t\t\t\t\trandom_state=1)\n",
    "\t\t\n",
    "\t\tclosed_clf.fit(X_train, y_train)\n",
    "\n",
    "\t\tself.closed_pred \t\t= closed_clf.predict(X_test)\n",
    "\t\tself.closed_accuracy \t= accuracy_score(y_test, self.closed_pred)\n",
    "\t\tself.closed_f1 \t\t\t= f1_score(y_test, self.closed_pred)\n",
    "\t\tself.closed_fnr, \n",
    "\t\tself.closed_recall, \n",
    "\t\tself.closed_precision, \n",
    "\t\tself.closed_specificity\t= ratios(y_test, self.closed_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "### Convert to Serialisable\n",
    "This simple helper function takes an entity as a paramater and returns it, after having been converted to a JSON serialisable type. Given the only abstract type being used is a **Numpy Int32**, there only needs to be one conditional. Every other type can be converted at base value.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_serialisable(ent):\n",
    "    if isinstance(ent, np.int32):\n",
    "        return int(ent)\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write JSON\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(class_name, output_file):\n",
    "\tclass_dict = {\n",
    "\t\t\"features\": {\n",
    "\t\t},\n",
    "\t\t\"results\": {\n",
    "\t\t\t\"accuracy\": class_name.closed_accuracy,#float\n",
    "\t\t\t\"f1\": class_name.closed_f1,#float\n",
    "\t\t\t\"fnr\": class_name.closed_fnr,#float\n",
    "\t\t\t\"recall\": class_name.closed_recall,#float\n",
    "\t\t\t\"precision\": class_name.closed_precision,#float\n",
    "\t\t\t\"specificity\": class_name.closed_specificity#float\n",
    "\t\t},\n",
    "\t\t\"predictions\": {\n",
    "\t\t\t\"y_pred\": str(class_name.closed_pred.tolist()),#array\n",
    "\t\t\t\"y_true\": str(y_test.tolist())#array\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\tif(isinstance(class_name, svm_model)):\n",
    "\t\tclass_dict[\"features\"] = {\n",
    "\t\t\t\"kernel\": str(class_name.kernel),#str\n",
    "\t\t\t\"max_iter\": class_name.max_iter,#int\n",
    "\t\t\t\"decision_function_shape\": str(class_name.func_shape),#str\n",
    "\t\t\t\"probability\": class_name.probability,#bool\n",
    "\t\t\t\"shrinking\": class_name.shrinking,#bool\n",
    "\t\t\t\"tollerance\": class_name.tollerance,#float\n",
    "\t\t\t\"C\": class_name.C#float\n",
    "\t\t}\n",
    "\telif(isinstance(class_name, ada_model)):\n",
    "\t\tclass_dict[\"features\"] = {\n",
    "\t\t\t\"Estimator\": {\n",
    "\t\t\t\t\"n_estimators\": class_name.clf_estimator['n_estimators'],#int\n",
    "\t\t\t\t\"criterion\": str(class_name.clf_estimator['criterion']),#str\n",
    "\t\t\t\t\"max_features\": str(class_name.clf_estimator['max_features']),#str\n",
    "\t\t\t\t\"bootstrap\": class_name.clf_estimator['bootstrap'],#bool\n",
    "\t\t\t\t\"min_samples_split\": class_name.clf_estimator['min_samples_split'],#int\n",
    "\t\t\t\t\"min_samples_leaf\": class_name.clf_estimator['min_samples_leaf']#int\n",
    "\t\t\t},\n",
    "\t\t\t\"Params\": {\n",
    "\t\t\t\t\"n_estimators\": class_name.clf_params['n_estimators'],#int\n",
    "\t\t\t\t\"learning_rate\": class_name.clf_params['learning_rate'],#float\n",
    "\t\t\t\t\"algorithim\": str(class_name.clf_params['algorithim'])#str\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\n",
    "\n",
    "\n",
    "\t_tojson_ = json.dumps(class_dict, default=convert_to_serialisable, indent=4)\n",
    "\n",
    "\twith open(output_file, 'a') as f:\n",
    "\n",
    "\t\tf.write(str(_tojson_ ) + ',\\n')\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_worst(current_test, solution_name):\n",
    "\n",
    "\tfor i in range(len(solution_list)):\n",
    "\n",
    "\t\tif current_test.closed_fnr < accuracy_list[i]:\n",
    "\t\t\tsolution_list.append(current_test)\n",
    "\t\t\taccuracy_list.append(current_test.closed_fnr)\n",
    "\n",
    "\t\t\twrite_json(current_test, solution_name)\n",
    "\n",
    "\n",
    "\n",
    "\t\t\tif(len(solution_list) > 10):\n",
    "\t\t\t\tsolution_list.pop(0)\n",
    "\t\t\t\taccuracy_list.pop(0)\n",
    "\n",
    "\t\t\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_best(current_test, solution_name):\n",
    "\n",
    "\tfor i in range(len(solution_list)):\n",
    "\n",
    "\t\tif current_test.closed_fnr > accuracy_list[i]:\n",
    "\t\t\tsolution_list.append(current_test)\n",
    "\t\t\taccuracy_list.append(current_test.closed_fnr)\n",
    "\n",
    "\t\t\twrite_json(current_test, solution_name)\n",
    "\n",
    "\t\t\tif(len(solution_list) > 10):\n",
    "\t\t\t\tsolution_list.pop(0)\n",
    "\t\t\t\taccuracy_list.pop(0)\n",
    "\n",
    "\t\t\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svmDepthFirstSearch(data):\n",
    "\n",
    "\titerationCount = 0\n",
    "\n",
    "\tnumber_of_tests = (\n",
    "\t\tlen(data['kernel']) *\n",
    "\t\tlen(data['decision_function_shape']) *\n",
    "\t\tlen(data['possibility']) *\n",
    "\t\tlen(data['possibility']) *\n",
    "\t\tlen(data['max_iter']) *\n",
    "\t\tlen(data['tollerance']) *\n",
    "\t\tlen(data['C'])\n",
    "\t)\n",
    "\n",
    "\twith open(\"svm_results.json\", 'w') as f:\n",
    "\t\tf.write(\"[\\n\")\n",
    "\t\t\n",
    "\tsolution_list.append(current_svm_data)\n",
    "\taccuracy_list.append(1.0)\n",
    "\n",
    "\tfor kernel in data['kernel']:\n",
    "\t\tfor function_shape in data['decision_function_shape']:\n",
    "\t\t\tfor probability in data['possibility']:\n",
    "\t\t\t\tfor shrinking in data['possibility']:\n",
    "\t\t\t\t\tfor max_iter in data['max_iter']:\n",
    "\t\t\t\t\t\tfor tol in data['tollerance']:\n",
    "\t\t\t\t\t\t\tfor C in data['C']:\n",
    "\n",
    "\t\t\t\t\t\t\t\tcurrent_svm_data['kernel'] = kernel\n",
    "\t\t\t\t\t\t\t\tcurrent_svm_data['decision_function_shape'] = function_shape\n",
    "\t\t\t\t\t\t\t\tcurrent_svm_data['probability'] = probability\n",
    "\t\t\t\t\t\t\t\tcurrent_svm_data['shrinking'] = shrinking\n",
    "\t\t\t\t\t\t\t\tcurrent_svm_data['max_iter'] = max_iter\n",
    "\t\t\t\t\t\t\t\tcurrent_svm_data['tollerance'] = tol\n",
    "\t\t\t\t\t\t\t\tcurrent_svm_data['C'] = C\n",
    "\n",
    "\t\t\t\t\t\t\t\tcurrent_test = svm_model(current_svm_data)\n",
    "\t\t\t\t\t\t\t\tcurrent_test.predict()\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\tcheck_worst(current_test, \"svm_results.json\")\n",
    "\n",
    "\t\t\t\t\t\t\t\titerationCount += 1\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\twith open(\"svm_iterations.txt\", 'w') as f:\n",
    "\t\t\t\t\t\t\t\tf.write(\"Iteration: \" + str(iterationCount) + \"/\" + str(number_of_tests))\n",
    "\n",
    "\n",
    "\twith open(\"svm_results.txt\", 'a') as f:\n",
    "\t\tf.write(\"]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaDepthSearch(data):\n",
    "\n",
    "\titerationCount = 0\n",
    "\n",
    "\tnumber_of_tests = (\n",
    "\t\tlen(data['Estimator']['n_estimators']) *\n",
    "\t\tlen(data['Estimator']['criterion']) *\n",
    "\t\tlen(data['Estimator']['max_features']) *\n",
    "\t\tlen(data['Estimator']['bootstrap']) *\n",
    "\t\tlen(data['Estimator']['min_samples_split']) *\n",
    "\t\tlen(data['Estimator']['min_samples_leaf']) *\n",
    "\t\tlen(data['Params']['n_estimators']) *\n",
    "\t\tlen(data['Params']['learning_rate']) *\n",
    "\t\tlen(data['Params']['algorithim'])\n",
    "\t)\n",
    "\n",
    "\twith open(\"ada_results.json\", 'w') as f:\n",
    "\t\tf.write(\"[\\n\")\n",
    "\t\t\n",
    "\tsolution_list.append(current_ada_data)\n",
    "\taccuracy_list.append(0.0)\n",
    "\n",
    "\tfor n_estimators in data['Estimator']['n_estimators']:\n",
    "\t\tfor criterion in data['Estimator']['criterion']:\n",
    "\t\t\tfor max_features in data['Estimator']['max_features']:\n",
    "\t\t\t\tfor bootstrap in data['Estimator']['bootstrap']:\n",
    "\t\t\t\t\tfor min_samples_split in data['Estimator']['min_samples_split']:\n",
    "\t\t\t\t\t\tfor min_samples_leaf in data['Estimator']['min_samples_leaf']:\n",
    "\n",
    "\t\t\t\t\t\t\tcurrent_ada_data['Estimator']['n_estimators'] = n_estimators\n",
    "\t\t\t\t\t\t\tcurrent_ada_data['Estimator']['criterion'] = criterion\n",
    "\t\t\t\t\t\t\tcurrent_ada_data['Estimator']['max_features'] = max_features\n",
    "\t\t\t\t\t\t\tcurrent_ada_data['Estimator']['bootstrap'] = bootstrap\n",
    "\t\t\t\t\t\t\tcurrent_ada_data['Estimator']['min_samples_split'] = min_samples_split\n",
    "\t\t\t\t\t\t\tcurrent_ada_data['Estimator']['min_samples_leaf'] = min_samples_leaf\n",
    "\n",
    "\n",
    "\t\t\t\t\t\t\tfor n_estimators_params in data['Params']['n_estimators']:\n",
    "\t\t\t\t\t\t\t\tfor learning_rate in data['Params']['learning_rate']:\n",
    "\t\t\t\t\t\t\t\t\tfor algorithim in data['Params']['algorithim']:\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t\t\tcurrent_ada_data['Params']['n_estimators'] = n_estimators_params\n",
    "\t\t\t\t\t\t\t\t\t\tcurrent_ada_data['Params']['learning_rate'] = learning_rate\n",
    "\t\t\t\t\t\t\t\t\t\tcurrent_ada_data['Params']['algorithim'] = algorithim\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\tcurrent_test = ada_model(current_ada_data)\n",
    "\t\t\t\t\t\t\t\t\t\tcurrent_test.predict()\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t\t\tcheck_best(current_test, \"ada_results.json\")\n",
    "\n",
    "\t\t\t\t\t\t\t\t\t\titerationCount += 1\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t\twith open(\"ada_iterations.txt\", 'w') as f:\n",
    "\t\t\t\t\t\t\t\t\t\tf.write(\"Iteration: \" + str(iterationCount) + \"/\" + str(number_of_tests))\n",
    "\n",
    "\n",
    "\twith open(\"ada_results.txt\", 'a') as f:\n",
    "\t\tf.write(\"]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmDepthFirstSearch(svm_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaDepthSearch(ada_ensambles_tests)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_SK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
